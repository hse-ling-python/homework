## Домашнее задание 4. Часть 1

**Тема: краулеры**

**Дедлайны:**
 
- 27.11 - 181, 182, 183 группы
- 30.11 - 184 группа

**Важное: работы принимаются строго в ipynb (не скрипт, не скрипт в ячейке ipynb)**

**Задание**

Вам нужно обкачать новости (или статьи) с сайтов газет или аналогичных онлайн-изданий и поместить это в базу. **Точная формулировка по необходимым данным будет от преподавателя в зависимости от информации на выбранном сайте**

**Данные**

Надо записаться в таблицу, чтобы ваши газеты не повторялись: [таблица](https://docs.google.com/spreadsheets/d/1Lhp7P-GPtzPrA8seYlZEO3lcDHLN4M9BmLCSxpje0FM/edit?usp=sharing)

**Совет по выбору**

Легче всего, если:

- навигация по страницам/выпускам/датам - под/над есть что-то вроде 1, 2, 3, ... 45, 46 по которым переход на страницы (например, Медуза - это плохой вариант, там кнопка "показать еще", The Village хороший вариант - там внизу навигация по страницам)
- в адресной строке будет навигация вроде <адрес>p=2 или  <адрес>page=2 или что-то такое, например, как на фикбуке ("https://ficbook.net/fanfiction/no_fandom/originals?p=2"). Это можно посмотреть, перейдя на вторую страницу интересующей ленты новостей.
- региональная газета - там попроще сайт и будет легче доставать информацию (у топовых газет много всего на скриптах и там сложно или невозможно доставать информацию)

**Схема базы**:

- таблица для текстов:
    - id
    - дата (отдельно поля год месяц день)
    - текст новости
    - категория (спорт, политика), если возможно
    - количество просмотров, если возможно
- таблица для какой-то дополнительной информации (уточнит преподаватель по вашему сайту), например как в семинаре: id, текст тега
- таблица связывающая две предыдущие, например: id, id_text, id_tag

Минимальные требования по объему: 1000 постов/новостей/статей, если не будет решено, что сайт сложный и можно меньше

**Пункты**:

1. Формальные требования:
    - все сохраняется в базу данных и ее схема соответствует схеме в задании
    - программа разделена на функции (например, функция для обхода ленты рубрики, функция для обработки страницы одной записи, см. конспект)
    - идет проверка на дублирование записей (не сохраняется дважды информация об одном объекте)
    - есть обработка ошибок (try-except) и куда-то сохраняется эта информация об ошибке (где и какая), чтобы можно было бы посмотреть, в чем проблема (и, может быть, перезапустить)
2. Выполнены требования по минимальному объему собранного материала
3. Посчитайте какую-то статистику по собранным данным (2 штуки), например:
    - (особенно хорошо) лемматизируйте тексты, сохраните в базе и постройте wordcloud по неделям/месяцам/рубрикам/тегам
    - посчитайте число записей по тегам (найдите популярные теги)
    - посчитайте число записей по авторам (найдите самых популярных авторов)
    - количетсво записей по дням

**Подсказка** используйте библиотеку tqdm для удобного отслеживания прогресса

**Чек-лист**

1. Тетрадка с кодом
2. **Получившаяся база данных (она должна открываться и быть заполненной!)**, проверьте на адекватность хотя бы по размеру файла - он должен быть как минимм несколько мегабайт

Если у вас есть вопросы, задавайте в чате или пишите преподавателям

### Критерии оценки

За это задание 4 балла (остальные "в предыдущих сериях")

<table>
    <tr><th>Балл</th><th>Критерий</th></tr>
    <tr><td>1 балл (по 0.5)</td><td>Пункт 3</td></tr>
    <tr><td>3 балла</td><td>Пункт 1 (пропорционально объему собранного материала - доля от минимального, если выполнен, то 1*балл, если 50%, то 0.5*балл и т.д.) </td></tr>
</table>


**Ссылки на GiHub Classroom: те же**
